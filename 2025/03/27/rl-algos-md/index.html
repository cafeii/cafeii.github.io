<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM+RL的基石：强化学习算法 | Welcome to Chen's Blog</title><meta name="author" content="Zhichen"><meta name="copyright" content="Zhichen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="刚入门llm的时候，觉得RL只是rlhf的工具，没有也无所谓，甚至由于对齐税我还有点负面印象。后来在Math等推理任务的深入，逐渐感受到了RL在推理上的重要性。o1出现让我意识到RL应该会是以后的突破口，于是捡回RL的知识。25年初，R1验证了我的猜想，好在我带着一点基础进来，勉强没被时代发展甩在后头。本文尝试记录一下我对rl算法的理解，希望能帮到有需要的人  现阶段大模型RL的发展，已经和原来">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM+RL的基石：强化学习算法">
<meta property="og:url" content="http://example.com/2025/03/27/rl-algos-md/index.html">
<meta property="og:site_name" content="Welcome to Chen&#39;s Blog">
<meta property="og:description" content="刚入门llm的时候，觉得RL只是rlhf的工具，没有也无所谓，甚至由于对齐税我还有点负面印象。后来在Math等推理任务的深入，逐渐感受到了RL在推理上的重要性。o1出现让我意识到RL应该会是以后的突破口，于是捡回RL的知识。25年初，R1验证了我的猜想，好在我带着一点基础进来，勉强没被时代发展甩在后头。本文尝试记录一下我对rl算法的理解，希望能帮到有需要的人  现阶段大模型RL的发展，已经和原来">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/images/waifu.png">
<meta property="article:published_time" content="2025-03-27T09:10:10.000Z">
<meta property="article:modified_time" content="2025-08-03T09:19:55.043Z">
<meta property="article:author" content="Zhichen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/waifu.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM+RL的基石：强化学习算法",
  "url": "http://example.com/2025/03/27/rl-algos-md/",
  "image": "http://example.com/images/waifu.png",
  "datePublished": "2025-03-27T09:10:10.000Z",
  "dateModified": "2025-08-03T09:19:55.043Z",
  "author": [
    {
      "@type": "Person",
      "name": "Zhichen",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/03/27/rl-algos-md/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM+RL的基石：强化学习算法',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background: [object Object];"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/waifu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/resume/"><i class="fa-fw fas fa-id-card"></i><span> Resume</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Welcome to Chen's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM+RL的基石：强化学习算法</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/resume/"><i class="fa-fw fas fa-id-card"></i><span> Resume</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LLM+RL的基石：强化学习算法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-03-27T09:10:10.000Z" title="Created 2025-03-27 17:10:10">2025-03-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-08-03T09:19:55.043Z" title="Updated 2025-08-03 17:19:55">2025-08-03</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p>刚入门llm的时候，觉得RL只是rlhf的工具，没有也无所谓，甚至由于对齐税我还有点负面印象。后来在Math等推理任务的深入，逐渐感受到了RL在推理上的重要性。o1出现让我意识到RL应该会是以后的突破口，于是捡回RL的知识。25年初，R1验证了我的猜想，好在我带着一点基础进来，勉强没被时代发展甩在后头。本文尝试记录一下我对rl算法的理解，希望能帮到有需要的人</p>
</blockquote>
<p>现阶段大模型RL的发展，已经和原来的RLHF没什么关系了，RL逐渐成为提高模型推理能力的重要算法。如果把sft等价为行为克隆（BC），那么交给模型自己去探索是优于BC人类行为的，因为我们无法保证人类设计的SFT轨迹，是算法上最优的轨迹。alphazero证明了棋类任务上agent学习人类的行为不如agent自己探索，r1-zero也证明了推理任务上学习人类设计的轨迹不如agent自己推理</p>
<p>所以为了学习大模型RL，最好抛开所有的LLM算法的知识：从现在开始，模型不叫模型，叫agent；sft不叫sft，叫BC；token从现在开始没有意义，我们只关注state＆action；梯度下降也一边去，我们只关注梯度上升。抛开LLM算法的知识污染，以空白的大脑理进入大模型RL</p>
<h3 id="强化学习基础组件"><a href="#强化学习基础组件" class="headerlink" title="强化学习基础组件"></a>强化学习基础组件</h3><p>RL的基础包含状态s，动作a，奖励r，策略θ</p>
<p>与LLM中的知识对比：</p>
<ul>
<li>s→状态，理解为模型的输入</li>
<li>a→动作，理解为模型的输出，可以是token、sentence、CoT…取决于问题如何定义</li>
<li>r→无</li>
<li>θ→模型本身，理解为一个策略函数，或者一个智能体</li>
</ul>
<p>RL学习分为探索和利用，探索会尝试更多的决策来发现，利用会更多的利用现有的策略。不管是探索还是利用，我们都需要对智能体所处环境进行估计，也就是值函数，代表了对环境的建模。 V(s) 是<strong>状态值函数</strong>，是对状态s的打分， Q(s,a) 是<strong>状态-动作值函数</strong>，是对状态s下做出的动作a的打分，他们统称<strong>价值函数</strong></p>
<h3 id="Value-based-Policy-based"><a href="#Value-based-Policy-based" class="headerlink" title="Value-based &amp; Policy-based"></a>Value-based &amp; Policy-based</h3><p>我们跳过了一个叫做贝尔曼最优方程的东西，它的用处就是找到最优的价值函数</p>
<p><img src="https://picx.zhimg.com/80/v2-828269ac0aeb0d42c757ad8efa9ff62e_720w.png?source=d16d100b" alt="img"></p>
<p>最优动作值函数</p>
<p>从而直接进行决策（选最大的那个），这就是<strong>价值迭代</strong>，整个过程没有策略，或者说策略就是最大化价值</p>
<p>我们给价值函数加上一个策略π，现在我们的问题变成了评估当前策略π</p>
<p><img src="https://pic1.zhimg.com/80/v2-395259de614069b0ae648b08821d0a0c_720w.png?source=d16d100b" alt="img"></p>
<p>加上策略π</p>
<p>然后用greedy的方式改进策略，这就是<strong>策略迭代</strong></p>
<p><img src="https://picx.zhimg.com/80/v2-aabb6fca5d4294755faa5fad7b6b851b_720w.png?source=d16d100b" alt="img"></p>
<p>当然也有其它的改进方法，例如策略梯度</p>
<p>LLM是一个天然的策略模型，它给定状态，然后输出动作的概率（把LLM从词表中采样一个token、或者一整个CoT的过程看成一个动作）。LLM也可以当价值模型，但是使用方式就完全不一样了</p>
<p>然后我们记住两个方法，一个是蒙特卡洛（MC），一个是时序差分（TD）。如果问题很大，我们没法直接迭代贝尔曼方程，例如围棋，就用他们估计值函数</p>
<p>MC的理论就是，随机采样一条轨迹，走到底，然后回头计算这条轨迹上每个state（或者state-action）的reward，以此来更新对值函数的估计</p>
<p>TD的理论就是，一次只走一步，然后以这一步的reward，更新对值函数的估计。走一步就是TD(0)，多走几步就是TD( λ )，全走完就是MC</p>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>在进入AC前，我们得说清楚RL中的模型</p>
<p>在与环境的交互过程中，我们一定知道s和a，我们一定不知道价值函数，因此我们要去学习一个价值函数。如果问题简单我们可以查表更新表，像Q-Table，但如果问题难，我们只能近似，也就是学一个value model。如果我们连reward都不知道，我们就得学习一个reward model。如果我们使用policy-based，通常也要有一个policy model</p>
<p><img src="https://picx.zhimg.com/80/v2-e8b4f14e5213f17b2fd2d1d40a329270_720w.png?source=d16d100b" alt="img"></p>
<p>对于policy model πθ 的更新，我们采用梯度上升法，即对于打分高的动作，我鼓励它，对于打分不高的动作，我不鼓励它</p>
<p><img src="https://picx.zhimg.com/80/v2-fe259983fd4aadae313b32d2ec9b8367_720w.png?source=d16d100b" alt="img"></p>
<p>基于Q-value的梯度提升</p>
<p>很快我们发现，这样不好，因为整体上不管做出什么动作，我一直在鼓励它，要等很久模型才会发现，有些动作我相对不是那么鼓励他，他才去调整策略。总之直接打分效率低</p>
<p>于是引入了AC。AC说，πθ 是actor，他的动作的打分函数是Q，然后再来一个critic评价actor的策略，他的评分是V，于是我们有优势函数A&#x3D;Q-V。用Q-V，我们把原来绝对的Q转换成相对的优势值A，它就有好有坏了</p>
<p><img src="https://picx.zhimg.com/80/v2-59efbbf5516e1c1882098351b64547a6_720w.png?source=d16d100b" alt="img"></p>
<p>我们需要估计A。但是AC发现，Q可以推导成V</p>
<p><img src="https://pic1.zhimg.com/80/v2-beb323ab9e35c86d826810936accd9b7_720w.png?source=d16d100b" alt="img"></p>
<p>一通推导之后，发现估计A和估计V是等价的，所以AC用TD近似A，这样我们只需要更新V就可以了，不需要额外的Q网络</p>
<p><img src="https://picx.zhimg.com/80/v2-93a1176d4fe67e9a964a458249d329f6_720w.png?source=d16d100b" alt="img"></p>
<p>用这个更新V</p>
<p>接着用梯度上升来更新π</p>
<p><img src="https://picx.zhimg.com/80/v2-390509c91fa9b3306ad2a67da319fc8c_720w.png?source=d16d100b" alt="img"></p>
<p>我们原本用这个更新π</p>
<p><img src="https://pic1.zhimg.com/80/v2-cc698ced7ff7523ac59459205b4ad99c_720w.png?source=d16d100b" alt="img"></p>
<p>在用TD近似A之后，我们用这个更新π</p>
<blockquote>
<p>AC真是从textbook走向practice的门槛，写这段的时候疯狂翻书</p>
</blockquote>
<h3 id="理解baseline"><a href="#理解baseline" class="headerlink" title="理解baseline"></a>理解baseline</h3><p>话又说回来，我们为什么要AC？</p>
<p>因为比起绝对值，我们更喜欢相对值</p>
<p>考虑三个动作，Qa1&#x3D;10，Qa2&#x3D;9，Qa3&#x3D;11，如果直接拿它们来梯度上升，三个动作的上升幅度看起来都差不多，他们之间的优势差距并不明显</p>
<p>此时我引入一个状态值V，V&#x3D;10，那就分得出优劣了。当前的状态已经是10分了，a1从10分到10分，啥也没干；a2从10分到9分，开倒车；a3从10分到11分，这个是好的，选它</p>
<p>我们把原来的Q1&#x3D;10，Q2&#x3D;9，Q3&#x3D;11，变成了A1&#x3D;0，A2&#x3D;-1，A3&#x3D;1，我们更喜欢这样的梯度信息，它更明确</p>
<p>那我这个梯度更新方向的来源，他一定得是AC吗？</p>
<p>我们先来看看有哪些东西可以用</p>
<p><img src="https://pic1.zhimg.com/80/v2-fda1c13841c8bf95bca4e952a44246df_720w.png?source=d16d100b" alt="img"></p>
<p>出处：GAE论文</p>
<ol>
<li>轨迹的总回报</li>
<li>当前动作之后的总回报</li>
<li>当前动作之后的总回报减去一个baseline</li>
<li>Q-value</li>
<li>优势函数，Q-V</li>
<li>V-value的时序差分，它可以近似优势函数</li>
</ol>
<p>我好像不一定非要用AC啊</p>
<p>我只是想要一个相对值</p>
<p>如果我有reward的话，直接用reward减去baseline行不行？</p>
<p>我反正只是要一个baseline，我不是以上几种行不行？</p>
<p>下面就要进入具体的算法</p>
<h3 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h3><p>ppo用一个叫广义优势估计（GAE）的东西指导梯度更新，长这样</p>
<p><img src="https://picx.zhimg.com/80/v2-5eba96b5d61caf1853a208db80b9675f_720w.png?source=d16d100b" alt="img"></p>
<p><img src="https://picx.zhimg.com/80/v2-1415ff6d4a2564bdab4760bb0fd75fa4_720w.png?source=d16d100b" alt="img"></p>
<p>可以看成TD和总回报加权了，这就是优势函数</p>
<p>然后再看PPO的算法</p>
<p>在PPO的前面一个工作，TRPO，引入了一个叫trust region的东西</p>
<p><img src="https://pic1.zhimg.com/80/v2-915bb700c72c6705832371af61d1768b_720w.png?source=d16d100b" alt="img"></p>
<p>总的来说就是不想让模型的更新过于偏离原来的参数</p>
<p>其中 π&#x2F;πold 是重要性采样</p>
<p>为什么要做重要性采样？这是一个off-policy的问题。我们让agent每次跟环境交互完之后马上更新策略，这是on-policy，但是现在我说不可以了，每n次交互才能更新一次</p>
<p>于是一个想法就是，我用 πold 产生数据优化策略，得到π0，π0得到π1，π1得到π2……直到又能更新了，再把最新的π给πold，重复这个过程。因此需要用当前策略相对旧策略的proportion，衡量当前策略相比旧策略有多少提升</p>
<p><img src="https://picx.zhimg.com/80/v2-4151cf35cae74ebc01280d245a5d75e9_720w.png?source=d16d100b" alt="img"></p>
<p>于是得到了基础的公式，在此基础之上，做出如下约束：</p>
<p><img src="https://picx.zhimg.com/80/v2-bfd55646abccd388f60104b335a64a3f_720w.png?source=d16d100b" alt="img"></p>
<p>我们就得到了PPO的算法，PPO-clip和PPO-kl</p>
<p>接着OpenAI就带着PPO一脚踏进了大模型的大门，RLHF</p>
<p>当时的PPO需要解决一个问题，就是大模型的输出不符合人类的偏好，比如我问你谁是第一位美国总统，我希望你直接说华盛顿，而不是讲一堆乱七八糟的美国历史</p>
<p>因此PPO面临的第一个问题就是，怎样能反映人类偏好，当然答案我们都知道了，用BT-score学习一个人类偏好的reward model，给大模型的输出打分，来近似真实环境下人类的reward</p>
<p>拿到reward model之后，我们手上有一个刚刚sft完的m0</p>
<p>我们知道GAE需要一个value model，这个value model来自于m0</p>
<p>策略模型，π，πold，也来自于m0</p>
<p>我们就可以进行PPO了</p>
<p>这就是RLHF的四个模型的来源</p>
<h3 id="DPO"><a href="#DPO" class="headerlink" title="DPO"></a>DPO</h3><p>DPO发现这么一件事，对于RLHF的目标来说</p>
<p><img src="https://pica.zhimg.com/80/v2-55a2d9a70e4f7f876e0aeb62599a26b2_720w.png?source=d16d100b" alt="img"></p>
<p>我们可以直接写出它的最优解</p>
<p><img src="https://picx.zhimg.com/80/v2-30ad0ce5ef8265e5f215b1f4b2d5096f_720w.png?source=d16d100b" alt="img"></p>
<p><img src="https://pic1.zhimg.com/80/v2-6267c46514731d8c84860d6b32f5ae13_720w.png?source=d16d100b" alt="img"></p>
<p>那么reward就可以写成</p>
<p><img src="https://pic1.zhimg.com/80/v2-854624eb42037f702540ccfa7a68c5cd_720w.png?source=d16d100b" alt="img"></p>
<p>我们用BT-score来学习人类偏好，本质是在学习这件事</p>
<p><img src="https://pic1.zhimg.com/80/v2-1931343020315509d731338f87b068ad_720w.png?source=d16d100b" alt="img"></p>
<p>我们把reward替进去</p>
<p><img src="https://pica.zhimg.com/80/v2-5196f1682106d6a9521b3575da7f39da_720w.png?source=d16d100b" alt="img"></p>
<p>接着就得到了目标函数</p>
<p><img src="https://pic1.zhimg.com/80/v2-8798b31fff20e653beb54f0842209ce5_720w.png?source=d16d100b" alt="img"></p>
<p>求导得到梯度</p>
<p><img src="https://picx.zhimg.com/80/v2-25cbeab92116cf6b6dc10aa769469972_720w.png?source=d16d100b" alt="img"></p>
<p><img src="https://pic1.zhimg.com/80/v2-7bb3b28a999654adbe9fa15a0dd92f79_720w.png?source=d16d100b" alt="img"></p>
<p>DPO成功的把模型从四个减少到了两个，其中 πref 不需要你训练，实质上只需要训练一个模型，数据也只需要提前收集好偏好数据，不需要实时采样</p>
<p>然而in practice，作为一个offline的算法，DPO还是比不过那些正经和环境做交互的online算法</p>
<h3 id="GRPO"><a href="#GRPO" class="headerlink" title="GRPO"></a>GRPO</h3><p>回到PPO，我们前面说过，不管是AC，还是GAE，本质上我只是想要一个baseline来对我的reward进行归一化，从而提高梯度上升的效率。那我必须要一个非常准确的estimator吗？我只想知道一个rough的相对关系，能够指导我的优化方向就行，可不可以？</p>
<p>GRPO非常巧妙的用group normalization解决了这个问题</p>
<p><img src="https://picx.zhimg.com/80/v2-959432833c6a6da91d75aa7d4c611520_720w.png?source=d16d100b" alt="img"></p>
<p>从对比图我们看到，PPO的流程是，对于一个question，采样一个output，然后对这个output计算GAE。因为一个sample非常珍贵，我们希望得到的优化方向越正确越好</p>
<p>但是GRPO很豪气的采样了好多个output，每个output都有一个reward，自然有好的output有坏的output，有高的reward有低的reward，我在这组数据上面归一化，自然知道了好的优化方向、坏的优化方向，就可以鼓励好的策略，打压坏的策略</p>
<p>于是GRPO成功的丢掉了value model</p>
<p>今天我们看到了R1，知道了reward model也可以换成rule-based reward，此时GRPO已经全方面碾压DPO，DPO半只脚踩进了历史的垃圾堆</p>
<h3 id="RLOO"><a href="#RLOO" class="headerlink" title="RLOO"></a>RLOO</h3><p><img src="https://pica.zhimg.com/80/v2-4645c58c259186ad21907b24203e80d5_720w.png?source=d16d100b" alt="img"></p>
<p>RLOO是与GRPO同期的工作，想法与GRPO类似，都想要丢掉value model。不同的是RLOO在计算优势的时候，当前response的优势值是当前的reward减去组内其他response的reward的mean</p>
<h3 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE++"></a>REINFORCE++</h3><p>RF++吸收了前几个算法的优点</p>
<p><img src="https://picx.zhimg.com/80/v2-136c4aebb542d70bb6056a9ea7d58223_720w.png?source=d16d100b" alt="img"></p>
<p>RF++以reward减去kl正则作为每个input-output数据的优势值，注意，此时的优势值不是经过归一化的reward</p>
<p><img src="https://pic1.zhimg.com/80/v2-9a0f00b730ad02812681969eed6344a9_720w.png?source=d16d100b" alt="img"></p>
<p>归一化的操作在这里：与grpo和rloo在group内对reward归一化计算得到优势值不同，rf++是对前面reward－kl得到的优势值在batch维度进行归一化</p>
<p>以PPO的loss作为优化目标</p>
<p><img src="https://picx.zhimg.com/80/v2-f5d80a1d1c7299230e99cb3cc221d6bb_720w.png?source=d16d100b" alt="img"></p>
<p>作者本人也在知乎发文宣传，RF++的结果比GRPO稳定比PPO快，不过像GRPO有Deepseek亲自背书，RF++需要更多大规模实验来走进大众视野</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Zhichen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2025/03/27/rl-algos-md/">http://example.com/2025/03/27/rl-algos-md/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/images/waifu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/05/torch-do-not-expand-segment/" title="torch踩坑：expandable_segments不要开"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">torch踩坑：expandable_segments不要开</div></div><div class="info-2"><div class="info-item-1">torch踩坑：expandable_segments不要开PYTORCH_CUDA_ALLOC_CONF里expandable_segments这个选项尽量不要开 在8*l40上开zero2跑微调遇到loss为nan的情况，虽然不知道原理为何，但我确信是因为开启了这个选项导致训练过程极度不稳定，并且segment会导致训练效率低下，基本与zero3没有区别 </div></div></div></a><a class="pagination-related" href="/2025/03/04/open-instruct-bug/" title="记一个使用open-instruct微调llama3-8b出现oom的bug"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">记一个使用open-instruct微调llama3-8b出现oom的bug</div></div><div class="info-2"><div class="info-item-1">记一个使用open-instruct微调llama3-8b出现oom的bug 起因是tulu3的微调，我发现之前正常使用的open-instruct的sft脚本突然出现oom，百思不得其解，寻思可能是赛博闹鬼，给服务器重启了一下，仍然闹鬼 唯物主义者不搞封建迷信，我坚信是两次启动sft的过程中哪个设置变动了，由于没有存log，我根本想不起我改动了什么，遂逐一筛查可能导致此问题的bug，最终锁定在数据处理过程中，传入到dataset mapping的preprocessing_num_workers这个参数上。这个值原先设置的是16，后改为32，被我忘记了，然后出现oom 为什么数据处理的worker数会影响gpu的显存？我想破脑袋也想不清这里面的关联。只能说以后要养成个好习惯，每次实验都要把参数给记录下来 </div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/waifu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Zhichen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/cafeii" target="_blank" title="fab fa-github"><i class="GitHub"></i></a><a class="social-icon" href="/12432685@mail.sustech.edu.cn" target="_blank" title="fas fa-envelope"><i class="E-Mail"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6"><span class="toc-number">1.</span> <span class="toc-text">强化学习基础组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Value-based-Policy-based"><span class="toc-number">2.</span> <span class="toc-text">Value-based &amp; Policy-based</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Actor-Critic"><span class="toc-number">3.</span> <span class="toc-text">Actor-Critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%90%86%E8%A7%A3baseline"><span class="toc-number">4.</span> <span class="toc-text">理解baseline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PPO"><span class="toc-number">5.</span> <span class="toc-text">PPO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DPO"><span class="toc-number">6.</span> <span class="toc-text">DPO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GRPO"><span class="toc-number">7.</span> <span class="toc-text">GRPO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RLOO"><span class="toc-number">8.</span> <span class="toc-text">RLOO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#REINFORCE"><span class="toc-number">9.</span> <span class="toc-text">REINFORCE++</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/05/torch-do-not-expand-segment/" title="torch踩坑：expandable_segments不要开">torch踩坑：expandable_segments不要开</a><time datetime="2025-07-05T08:00:10.000Z" title="Created 2025-07-05 16:00:10">2025-07-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/27/rl-algos-md/" title="LLM+RL的基石：强化学习算法">LLM+RL的基石：强化学习算法</a><time datetime="2025-03-27T09:10:10.000Z" title="Created 2025-03-27 17:10:10">2025-03-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/04/open-instruct-bug/" title="记一个使用open-instruct微调llama3-8b出现oom的bug">记一个使用open-instruct微调llama3-8b出现oom的bug</a><time datetime="2025-03-04T13:55:17.000Z" title="Created 2025-03-04 21:55:17">2025-03-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/23/sparse-attention-md/" title="稀疏注意力：从MoBA到NSA">稀疏注意力：从MoBA到NSA</a><time datetime="2025-02-23T01:29:27.000Z" title="Created 2025-02-23 09:29:27">2025-02-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/15/byte-latent-transformer-md/" title="阅读一下Byte Latent Transformer">阅读一下Byte Latent Transformer</a><time datetime="2024-12-15T03:53:48.000Z" title="Created 2024-12-15 11:53:48">2024-12-15</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Zhichen</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>